---
title: "Group Project"
author: "Dinesh Padmanabhan, Jake Lieberfarb,
Luis Ahumada, Noah Feldman, Becca Blacker
"
date: "3/09/2020"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(warning = F, results = T, message = T, echo = F)
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```


```{r Functions, include=FALSE} 
loadPkg = function(pkg, character.only = FALSE) { 
  if (!character.only) { pkg <- as.character(substitute(pkg)) }
  if (!require(pkg,character.only=T, quietly =T)) {  install.packages(pkg,dep=T,repos="http://cran.us.r-project.org"); if(!require(pkg,character.only=T)) stop("Package not found") } 
}
loadPkg(knitr)
```

```{r Remove Outliers, include=FALSE} 
# Fix outliers

outlierKD2 <- function(df, var, rm=FALSE) { 
    #' Original outlierKD functino by By Klodian Dhana,
    #' https://www.r-bloggers.com/identify-describe-plot-and-remove-the-outliers-from-the-dataset/
    #' Modified to have third argument for removing outliers inwtead of interactive prompt, 
    #' and after removing outlier, original df will not be changed. The function returns the new df, 
    #' which can be saved as original df name if desired.
    #' Check outliers, and option to remove them, save as a new dataframe. 
    #' @param df The dataframe.
    #' @param var The variable in the dataframe to be checked for outliers
    #' @param rm Boolean. Whether to remove outliers or not.
    #' @return The dataframe with outliers replaced by NA if rm==TRUE, or df if nothing changed
    #' @examples
    #' outlierKD2(mydf, height, FALSE)
    #' mydf = outlierKD2(mydf, height, TRUE)
    #' mydfnew = outlierKD2(mydf, height, TRUE)
    dt = df # duplicate the dataframe for potential alteration
    var_name <- eval(substitute(var),eval(dt))
    na1 <- sum(is.na(var_name))
    m1 <- mean(var_name, na.rm = T)
    #par(mfrow=c(2, 2), oma=c(0,0,3,0))
    boxplot(var_name, main="With outliers")
    hist(var_name, main="With outliers", xlab=NA, ylab=NA)
    outlier <- boxplot.stats(var_name)$out
    mo <- mean(outlier)
    var_name <- ifelse(var_name %in% outlier, NA, var_name)
    boxplot(var_name, main="Without outliers")
    hist(var_name, main="Without outliers", xlab=NA, ylab=NA)
    title("Outlier Check", outer=TRUE)
    na2 <- sum(is.na(var_name))
    cat("Outliers identified:", na2 - na1, "\n")
    cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(var_name))*100, 1), "\n")
    cat("Mean of the outliers:", round(mo, 2), "\n")
    m2 <- mean(var_name, na.rm = T)
    cat("Mean without removing outliers:", round(m1, 2), "\n")
    cat("Mean if we remove outliers:", round(m2, 2), "\n")
    
    # response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
    # if(response == "y" | response == "yes"){
    if(rm){
        dt[as.character(substitute(var))] <- invisible(var_name)
        #assign(as.character(as.list(match.call())$dt), dt, envir = .GlobalEnv)
        cat("Outliers successfully removed", "\n")
        return(invisible(dt))
    } else {
        cat("Nothing changed", "\n")
        return(invisible(df))
    }
}

```

```{r Load 2015 data, include=FALSE}
loadPkg('dplyr')
loadPkg('readxl')
loadPkg('readr')

#Read in general census data
census_2015 <- read.csv(file = 'acs2015updated.csv')

#Read in Specifically Education Data
edu_data_2015 <- read_csv("ACS_15_5YR_B15003_renamed.csv", skip = 1) %>% mutate(Id2 = as.numeric(Id2))

#Read in Freedom Data
freedom <- read_excel("Freedom_In_The_50_States_2018.xlsx", sheet = "Overall")

#Combine
census_edu_2015 <- left_join(census_2015, edu_data_2015, by = c("CensusTract" = "Id2"))
full_2015 <- full_join(census_edu_2015, freedom %>% filter(Year == 2015), by = "State") 

#For the education data, we will need to divide the count by the total
full_2015_eduProp <- full_2015 %>% mutate(PropHighSchool = EstHighSchool/EstTotal,
                                          PropBachelors = EstBachelors/EstTotal,
                                          PropDoctorate = EstDoctorate/EstTotal)
full_2015_varOfInterest <- full_2015 %>%  select(IncomePerCap,Hispanic:Asian,Professional:Production, SelfEmployed, Unemployment, FiscalPolicy,EconomicFreedom, RegulatoryPolicy, PersonalFreedom, OverallFreedom)

```

# Introduction 

America is a highly diverse country. It is not only diverse in terms of ethnicity, but also in terms of income, industry, and law. This opens the doors for a variety of possible interactions between these variables. What factors drive the way that income is distributed in the United States? What factors reliably predict whether the average income per capita in a specific area is high or low? How does state level variations in law and freedom impact income?

#How are these questions SMART?

These questions are important because they tell many facets of the story of consumption in the United States. Income serves both as a measure of productivity and lifetime consumption (although this analysis does not disentangle the two). Although their scope is broad, they remain specific to the concepts of income, demographics, and freedom, and maintain a consistent structure: how do demographics and freedom drive income in the United States, at the census tract level. 

These questions also correspond to a set of highly measurable (And luckily, premeasured) variables. Income can be imputed from tax records, while ethnicity and work status are available from census forms. The Freedom variables are somewhat more abstract, but come from an analysis from the Cato Institute (2015) which rigorously measures and weights different legal stances that each state might take. Achieving the answers to the questions is made simple by the cleanliness and availability of this data; since few data points are missing across all census tracts throughout the 50 states of interest, it is simple to form statistical tests.

Finally, these questions are relevant to policy makers who want to improve the incomes of their constituents as well as to researchers interested in establishing a baseline for the average income they should expect a community would earn based on its demographics. These are critical questions, because the ability of communities to support themselves economically has massive impacts on the wellbeing of their members.

# Content
First, an examination is conducted on how the US Census Bureau database is structured, and which variables were included. Secondly, an analysis of Cato Institute's Freedom in the 50 States shows whether or not it worked as a potential meaningful independent variable. Later on, the groups of independent variables and how each of them could affect the income per capita of a community is presented. Then, an exploratory data analysis and some statistical tests are made to evaluate the significance of our variables and a first preview of a regression analysis. Finally, a conclusion looks into further challenges and questions necessary to enhance future analyses.

# Dataset
## U.S. Census Bureau Dataset
The U.S. Census Bureau Data holds the yearly American Community Survey: a project which asks Americans around the country about several dimensions of their lives, including work, income, demographics, and other activities (U.S. Census Bureau, 2019). The dataset from 2015 was available via Kaggle (MuonNeutrino, 2015), and included more than 74,000 observations, with 37 columns (variables).
The dataset includes two variables related to income: the median household income and income per capita. The variable income per capita was prefered because it adjusts per person, and not per household given that it's unknown how many people can live in an average household. 
The variable income per capita (IncomePerCap) is calculated as the average income per capita of the population of a specific census tract. But, what is a census tract and why use them?

###Census tracts
Household's income in America varies significantly by geographical location. The richest counties in the country are concentrated in urban areas near big metropolises where most businesses are located. The bay area in northern California, Northeast Virginia and New York are some examples. However, counties have been an insufficient unit to compare different variables among them. There are 3,142 counties in a country of 300 million inhabitants (U.S. Census Bureau, 2019), but among them are several inconsistencies.
Texas, for example, has 254 counties (U.S. Census Bureau, 2017). California, a state with approximately 10 million people more than Texas, has only 58 counties (U.S. Census Bureau, 2017). Population-wise California has the largest county in the country with more than 10 million inhabitants (Los Angeles), whereas Texas has more than 80 counties with less than 10,000 people (U.S. Census Bureau, 2017). Density-wise, New York has 4 of 5  of the most dense counties in the country, some of them 60,000 times more dense than counties in Hawaii, Alaska or Nevada (U.S. Census Bureau, 2013).
As a response to these inconsistencies found in counties in America, the U.S. Census Bureau delineated "Census Tracts" at the beginning of the twentieth century.  A census tract is "geographic region defined for the purpose of taking a census." Over the years, the U.S. Census Bureau has established census tracts in every county in America. There are over 74,000 census tracts in the country and a typical one has around 4,000 or so residents. There is a strength that comes from this consistency: census tracts are by and large similar in population size, and the population size of census tracts does not vary much from state to state.

## Freedom in the 50 States
The Freedom in the 50 States project presents a ranking of the American states based on how their policies promote freedom in the fiscal, regulatory, and personal realms. It was published by the libertarian-leaning think tank Cato Institute in 2015 and receives biannual updates. The index gathers data on more than 230 variables to measure "local government intervention across a wide range of policy categories—from taxation to debt, from eminent domain laws to occupational licensing, and from drug policy to educational choice."
Before relying on this index, it is important to delve into the author's preferred definition of "freedom". Fortunately, the methodology of the index is very transparent about their criteria. The chapter dedicated to "defining freedom" in the report states: 

"We ground our conception of freedom on an individual rights framework. In our view, individuals should not be prevented from ordering their lives, liberties, and property as they see fit, so long as they do not infringe on the rights of others. [...]  This index attempts to measure the extent to which state and local public policies conform to this ideal regime of maximum, equal freedom. For us, the fundamental problem with state intervention in consensual acts is that it violates people’s rights." 

In other words, the report by the Cato Institute gives a higher score to those states whose laws protect individual rights, free markets, limited and small government and will punish those states with more regulations on trade and businesses, and restrictions on individual liberties.
According to this report, some examples that are included in the study on how restrictions on freedom, in an economic, personal, regulatory or fiscal level, would be:
- Aspiring professionals wanting to ply a trade without paying onerous examination and education costs.
- Less-skilled workers priced out of the market by minimum wage laws.
- Arrests for non drug victimless crimes, % of population
- Prohibition of same-sex partnership

### Why use this dataset as potential variables that affect income per capita?
It is valuable to determine how different combinations of public policies in these areas can affect income per capita. This allows to roughly establish if higher or lower levels of fiscal, regulatory or personal freedom (as defined by the Cato’s formulas) have any relationship with the income per capita level in a community.

# Merging datasets
The Freedom in the 50 States dataset, though, was calculated at the state, rather than census tract, level. Therefore, when merging, each census tract has freedom scores identical to the score at the overarching state level. So, for example, since the fiscal policy score for Alabama was 0.122, then all census tracts located in Alabama have  a “fiscal policy score” of 0.122. Unfortunately, this project does not allow the inspection of differences in policies below the state level (so policies enacted by municipalities are ignored). 


# Description of Variables
The complete dataset includes 17 independent variables and 1 dependent variable.
Thanks to their nature, the independent variables were classified in three groups: Freedom Scores, Work Variation and Ethnic Variation.

## Freedom Scores

###Overall Freedom:
The weighted sum of all the variables is used to produce the overall freedom ranking of the states. The overall freedom scores rate states on how free they are relative to other states. A score of 1 would correspond to a state’s being one standard deviation above average in every single variable, although in reality, every state scores better on some variables and worse on others. A score of 0 would be equivalent to a state’s being absolutely average on every variable, and a score of ¬1 to a state’s being one standard deviation below average on every variable (Cato Institute, 2018).

###Economic Freedom:
Economic freedom is calculated as the sum of the fiscal and regulatory freedom indices (Cato Institute, 2018).

###Personal Freedom:
The personal freedom consists of the following categories: (a) incarceration and arrests for victimless crimes, (b) gun rights, (c) gambling freedom, (d) marriage freedom, (e) educational freedom, (f) alcohol freedom, (g) asset forfeiture, (h) marijuana freedom, (i) tobacco freedom, (j) travel freedom, (k) campaign finance freedom, and (l) other mala prohibita and miscellaneous civil liberties (Cato Institute, 2018). 

###Regulatory Policy:
The regulatory policy dimension includes categories for land-use freedom and environmental policy, health insurance freedom, labor-market freedom, lawsuit freedom, occupational freedom, miscellaneous regulations that do not fit under another category (such as certificate of need requirements), and cable and telecommunications freedom (Cato Institute, 2018).

###Fiscal Policy:
The fiscal policy dimension consists of six variables: (a) state tax revenues, (b) government consumption, (c) local tax revenues, (d) government employment, (e) government debt, and (f) cash and security assets (Cato Institute, 2018).

##Work Variation:

###Professional:
Percentage (%) employed in management, business, science, and arts in a census tract.

###Service:
Percentage (%) employed in service jobs in a census tract.

###Office:
Percentage (%) employed in sales and office jobs in a census tract.

###Construction:
Percentage (%) employed in natural resources, construction, and maintenance in a census tract.

###Production:
Percentage (%) employed in production, transportation, and material movement in a census tract.

###Unemployed:
Unemployment rate (%) in a census tract.

###Self-employed:
Percentage (%) self-employed in a census tract.

##Ethnic Variation

###Native:
Percentage (%) of population that is Native American or Native Alaskan in a census tract.

###White:
Percentage (%) of population that is white in a census tract.

###Black:
Percentage (%) of population that is black in a census tract.

###Hispanic:
Percentage (%) of population that is Hispanic/Latino in a census tract.

###Asian:
Percentage (%) of population that is Asian in a census tract.



## Population Histogram and QQ

```{r population, include=FALSE} 
loadPkg("ggplot2")

# NA's
dat0<-full_2015[!is.na(full_2015$IncomePerCap),]
sum(is.na(dat0$IncomePerCap))

#removed outliers 
dat1 <- outlierKD2(dat0, IncomePerCap, TRUE)
#Histogram of dependent variable
hist(dat1$TotalPop, 
        col = "#a8c2fb",
        main = "population Count",
        xlab = "US Dollars")

#ggplot
qqnorm(dat1$TotalPop, main="Q-Q plot of TotalPop")
qqline(dat1$TotalPop)

#Mean after removing NA's
format(mean(dat1$TotalPop, na.rm = TRUE))

#Summary after removing NA's
summary(dat1$TotalPop)
#standard deviation
sd(dat1$TotalPop, na.rm = TRUE)

```
  A baseline analysis of population and income was conducted. The histogram for population appeared skewed to the right. The different census tracts had similar population counts with a mean of about 4000. Counties were not evenly spread out as some had a population of 1 million and others 10 million. With similar populations, census tracts were easier to investigate instead of counties. The Q-Q plot confirmed the non-normality as the values between quartiles 3 and 4 were far away from the line. 
```{r income including outliers, include=FALSE} 
#Histogram of dependent variable
hist(dat0$IncomePerCap, 
        col = "#a8c2fb",
        main = "Income per Capita",
        xlab = "US Dollars")

#ggplot
qqnorm(dat0$IncomePerCap, main="Q-Q plot of IncomePerCap")
qqline(dat0$IncomePerCap)

#Mean after removing NA's
format(mean(dat0$IncomePerCap, na.rm = TRUE))

#Summary after removing NA's
summary(dat0$IncomePerCap)
#standard deviation
sd(dat0$IncomePerCap, na.rm = TRUE)
```
## Income Histogram and QQ
```{r income no outliers} 
#Histogram of dependent variable
hist(dat1$IncomePerCap, 
        col = "#a8c2fb",
        main = "Income per Capita",
        xlab = "US Dollars")

#ggplot
qqnorm(dat1$IncomePerCap, main="Q-Q plot of IncomePerCap")
qqline(dat1$IncomePerCap)

#Mean after removing NA's
format(mean(dat1$IncomePerCap, na.rm = TRUE))

#Summary after removing NA's
summary(dat1$IncomePerCap)
#standard deviation
sd(dat1$IncomePerCap, na.rm = TRUE)
```
  The raw data for income appeared very skewed to the right as well. The data appeared to follow a power-law curve as some individuals have amassed a large amount of income and these outliers can skew the data. Thus, the outliers and NA values were removed; checking again, the “cleaned data” appeared normal. The histogram appears monomodal and the error terms along the Q-Q plot did not stray away from the line.
## Individual EDA of Freedom scores 
```{r Freedom scores} 
loadPkg("dvmisc")
#Group by Economic Freedom
GroupEF <- quant_groups(dat1$EconomicFreedom, 4)
str(GroupEF)

#Boxplot of IncomePerCap by Economic Freedom
plot(IncomePerCap ~ GroupEF, data=dat1, main="IncomePerCap and GroupEF", col=c("#ffd18b","#ffb76d","#ffa264", "#ff875f") )
summary(dat1$EconomicFreedom)

#Histogram of Economic Freedom
hist(dat1$EconomicFreedom, 
        col = "#ffd18b",
        main = "Economic Freedom")

#qqplot of Econoimc Freedom
qqnorm(dat1$EconomicFreedom, main="Q-Q plot of Economic Freedom")
qqline(dat1$EconomicFreedom)


#Group by PersonalFreedom
GroupPF <- quant_groups(dat1$PersonalFreedom, 4)
str(GroupPF)

#Boxplot of IncomePerCap by Personal Freedom
plot(IncomePerCap ~ GroupPF, data=dat1, main="IncomePerCap and GroupPF", col=c("#BFBFFF","#A3A3FF", "#7879FF", "#4949FF") )
summary(dat1$PersonalFreedom)

#Histogram of Personal Freedom
hist(dat1$PersonalFreedom, 
        col = "#A3A3FF",
        main = "Personal Freedom")

#qqplot of Personal Freedom
qqnorm(dat1$PersonalFreedom, main="Q-Q plot of Personal Freedom")
qqline(dat1$PersonalFreedom)

#Group by Regulatory Policy
GroupRP <- quant_groups(dat1$RegulatoryPolicy, 4)
str(GroupRP)

#Boxplot of IncomePerCap by Regulatory Policy
plot(IncomePerCap ~ GroupRP, data=dat1, main="IncomePerCap and GroupRP", col=c("#FFDF01","#FED901", "#FFCF00", "#FEC300") )
summary(dat1$RegulatoryPolicy)

#Histogram of Regulatory Policy
hist(dat1$RegulatoryPolicy, 
        col = "#FFDF01",
        main = "Regulatory Policy ")

#qqplot of Regulatory Policy
qqnorm(dat1$RegulatoryPolicy, main="Q-Q plot of Regulatory Policy")
qqline(dat1$RegulatoryPolicy)


#Group by Fiscal Policy
GroupFP <- quant_groups(dat1$FiscalPolicy, 4)
str(GroupFP)
#Boxplot of IncomePerCap by Fiscal Policy
plot(IncomePerCap ~ GroupFP, data=dat1, main="IncomePerCap and GroupFP", col=c("#7EFFD4","#70EBBA", "#64D8A7", "#58BD95") )
summary(dat1$FiscalPolicy)

#Histogram of Fiscal Policy
hist(dat1$FiscalPolicy, 
        col = "#7EFFD4",
        main = "Fiscal Policy")

#qqplot of Fiscal Policy
qqnorm(dat1$FiscalPolicy, main="Q-Q plot of Fiscal Policy")
qqline(dat1$FiscalPolicy)

#Group by Overall Freedom
GroupOF <- quant_groups(dat1$OverallFreedom, 4)
str(GroupOF)
#Boxplot of IncomePerCap by Overall Freedom
plot(IncomePerCap ~ GroupOF, data=dat1, main="IncomePerCap and GroupOF", col=c("#C0C6CB","#98A5C0", "#7688BB", "#536CB5") )
summary(dat1$OverallFreedom)

#Histogram of Overall Freedom
hist(dat1$OverallFreedom, 
        col = "#A3A3FF",
        main = "Overall Freedom")

#qqplot of Overall Freedom
qqnorm(dat1$OverallFreedom, main="Q-Q plot of Overall Freedom")
qqline(dat1$OverallFreedom)
```
  Next, the seventeen independent variables were analyzed. The freedom scores were  economic freedom, personal freedom, regulatory policy, fiscal policy, and overall freedom. The box plots were split up into four evenly distributed quartiles by the income per capita in each quartile. For all the five sets of boxplots, there did not appear to be any differences between the quartiles as they all overlapped roughly the same range of their respective independent variables. The histograms did not appear normal as overall the data was randomly spread out with huge gaps between bins. The Q-Q plots told a similar story as the error terms tended to follow a sin-like trend over the line and there were big tails on either end. None of the freedom scores appeared to be distributed normally.
## Individual EDA of Work Variations
```{r Work variations}
#Group by Unemployment
GroupUnemployment <- quant_groups(dat1$Unemployment, 4)
str(GroupUnemployment)
#Boxplot of IncomePerCap by Unemployment
plot(IncomePerCap ~ GroupUnemployment, data=dat1, main="IncomePerCap and GroupUnemployment", col=c("#FFF57B","#FFE469", "#FECC51", "#FCB033") )
summary(dat1$Unemployment)

#Histogram of Unemployment
hist(dat1$Unemployment, 
        col = "#A3A3FF",
       )

#qqplot of Unemployment Unemployment
qqnorm(dat1$Unemployment, main="Q-Q plot of Unemployment")
qqline(dat1$Unemployment)

#Group by Professional
GroupProfessional <- quant_groups(dat1$Professional, 4)
str(GroupProfessional)
#Boxplot of IncomePerCap by Professional
plot(IncomePerCap ~ GroupProfessional, data=dat1, main="IncomePerCap and GroupProfessional", col=c("#F3D8F2","#E6B2E4", "#D88DD5", "#CA68C7") )
summary(dat1$Professional)

#Histogram of Professional
hist(dat1$Professional, 
        col = "#D88DD5",
        main = "Professional")

#qqplot of Professional
qqnorm(dat1$Professional, main="Q-Q plot of Professional")
qqline(dat1$Professional)


#Group by Office
GroupOffice <- quant_groups(dat1$Office, 4)
str(GroupOffice)
#Boxplot of IncomePerCap by Office
plot(IncomePerCap ~ GroupOffice, data=dat1, main="IncomePerCap and GroupOffice", col=c("#D1FFD5","#B4FFB2", "#98FF98", "#79F58A") )
summary(dat1$Office)
#Histogram of Office
hist(dat1$Office, 
        col = "#FECC51",
        main = "Office")

#qqplot of Office
qqnorm(dat1$Office, main="Q-Q plot of Office")
qqline(dat1$Office)


#Group by Service
GroupService <- quant_groups(dat1$Service, 4)
str(GroupService)
#Boxplot of IncomePerCap by Service
plot(IncomePerCap ~ GroupService, data=dat1, main="IncomePerCap and GroupService", col=c("#E0BCBF","#D8ABB1", "#CF989F", "#C0838C") )
summary(dat1$Service)

#Histogram of Service
hist(dat1$Service, 
        col = "#79F58A",
        main = "Service")

#qqplot of Service
qqnorm(dat1$Service, main="Q-Q plot of Service")
qqline(dat1$Service)

#Group by Construction
GroupConstruction <- quant_groups(dat1$Construction, 4)
str(GroupConstruction)
#Boxplot of IncomePerCap by Construction
plot(IncomePerCap ~ GroupConstruction, data=dat1, main="IncomePerCap and GroupConstruction", col=c("#A9AB98","#949180", "#7D7968", "#5E594F") )
summary(dat1$Construction)
#Histogram of Construciton
hist(dat1$Construction, 
        col = "#C0838C",
        main = "Construction")

#qqplot of Construction
qqnorm(dat1$Construction, main="Q-Q plot of Construction")
qqline(dat1$Construction)

#Group by Production
GroupProduction <- quant_groups(dat1$Production, 4)
str(GroupProduction)
#Boxplot of IncomePerCap by Production
plot(IncomePerCap ~ GroupProduction, data=dat1, main="IncomePerCap and Production", col=c("#F3D8F2","#E6B2E4", "#D88DD5", "#CA68C7") )
summary(dat1$Production)
#Histogram of Producrtion
hist(dat1$Production, 
        col = "#E6B2E4",
        main = "Production")

#qqplot of Production
qqnorm(dat1$Production, main="Q-Q plot of Production")
qqline(dat1$Production)

#Group by Self-Employed
GroupProduction <- quant_groups(dat1$SelfEmployed, 4)
str(GroupProduction)
#Boxplot of IncomePerCap by Selfemployed
plot(IncomePerCap ~ GroupProduction, data=dat1, main="IncomePerCap and Selfemployed", col=c("#F3D8F2","#E6B2E4", "#D88DD5", "#CA68C7") )
summary(dat1$SelfEmployed)
#Histogram of Self Employed
hist(dat1$SelfEmployed, 
        col = "#E6B2E4",
        main = "Production")

#qqplot of Self employed
qqnorm(dat1$SelfEmployed, main="Q-Q plot of Selfemployed")
qqline(dat1$SelfEmployed)
```
  Next the seven variables for work variations (professional, production, unemployment, office, service, construction, self-employed) were assessed for normality. The boxplots that exhibited a decrease in income, as more of the specific work variation was included in the census tract, were unemployment, service, construction, and production. That is to say, as more unemployed individuals were accounted for in a given census tract, the income per capita decreased. The only work variation that exhibited an increase in average income was professional work. The remaining variables of office and self-employed remained relatively stable across quartiles. Looking at the histograms of each of the variables it appeared that only the proportion of professionals was distributed normally. The remaining six work variations were all skewed to the right. For professionals, the Q-Q plots affirmed the normality as the plot did not have the error terms straying far from the line with very small right and left tails. The same cannot be said for the other variables as each had an oversized right tail and a relatively small left tail. Overall the proportion of professionals appeared normally distributed while the other work variations did not.
## Individual EDA of ethnicities
```{r  Ethnic Variatons}
loadPkg("dvmisc")
#Group by Black
GroupBlack <- quant_groups(dat1$Black, 4)
str(GroupBlack)
#Boxplot of IncomePerCap by Black Population
plot(IncomePerCap ~ GroupBlack, data=dat1, main="IncomePerCap and GroupBlack", col=c("#d0dfff","#a8c2fb", "#86abf9", "#6893ee") )
summary(dat1$Black)

#Histogram of Black
hist(dat1$Black, 
        col = "#d0dfff",
        main = "Black")

#qqplot of Black
qqnorm(dat1$Black, main="Q-Q plot of Black")
qqline(dat1$Black)

#Group by Hispanic
GroupHispanic <- quant_groups(dat1$Hispanic, 4)
str(GroupHispanic)
#Boxplot of IncomePerCap by Hispanic Population
plot(IncomePerCap ~ GroupHispanic, data=dat1, main="IncomePerCap and GroupHispanic", col=c("#F6BDC0","#F1959B", "#F07470", "#EA4C46") )
summary(dat1$Hispanic)
#Histogram of Hispanic
hist(dat1$Hispanic, 
        col = "#EA4C46",
        main = "Hispanic")

#qqplot of Hispanic
qqnorm(dat1$Hispanic, main="Q-Q plot of Hspanic")
qqline(dat1$Hispanic)

#Group by Asians
GroupAsian <- quant_groups(dat1$Asian, 4)
str(GroupAsian)
#Boxplot of IncomePerCap by Asian Population
plot(IncomePerCap ~ GroupAsian, data=dat1, main="IncomePerCap and GroupAsian", col=c("#FFE6C8","#FFCCBE", "#EFABA0", "#D6806F") )
summary(dat1$Asian)
#Histogram of Asian
hist(dat1$Asian, 
        col = "#FFE6C8",
        main = "Asian")

#qqplot of Asian
qqnorm(dat1$Asian, main="Q-Q plot of Asian")
qqline(dat1$Asian)

#Group by White
GroupWhite <- quant_groups(dat1$White, 4)
str(GroupWhite)
#Boxplot of IncomePerCap by White Population
plot(IncomePerCap ~ GroupWhite, data=dat1, main="IncomePerCap and GroupWhite", col=c("#F5F8FA", "#EFF2F4", "#EAEDEF", "#E0E3E5") )
summary(dat1$White)
#Histogram of White
hist(dat1$White, 
        col = "#F5F8FA",
        main = "White")

#qqplot of White
qqnorm(dat1$White, main="Q-Q plot of White")
qqline(dat1$White)

#Group by native
#GroupNative <- quant_groups(dat1$Native, 4)
#str(GroupNative)

#Boxplot of IncomePerCap by Native Population
#plot(IncomePerCap ~ GroupNative, data=dat1, main="IncomePerCap and GroupNative", col=c("#F3D8F2","#E6B2E4", "#D88DD5", "#CA68C7") )
summary(dat1$Native)
#Histogram of Native
hist(dat1$Native, 
        col = "#F5F8FA",
        main = "Native")

#qqplot of Native
qqnorm(dat1$Native, main="Q-Q plot of Native")
qqline(dat1$Native)


```
  Finally the five ethnic variables (Native, White, Black, Hispanic, and Asian) were investigated. The boxplots for White showed an increase in average income between the first second and third quartiles but no change in the fourth. The boxplot for Asian showed an increase from the first through the fourth quartile. The boxplots for Hispanic slightly increased between the first and second quartile but did not change for the third quartile. The fourth quantile for Hispanic decreased significantly. The boxplot for Black increased in average income between the first and second quartile. Then there was a decrease in average income from the second to the fourth quartiles. Overall, it appeared that average income did change based on concentration of ethnicities in a census tract. The histogram for White was bimodal with the highest frequency at over 8,000. The histograms for the other four ethnicities were skewed to the right. Based on the histogram, it appeared that white had the highest responses followed by Hispanic, Black, Asian, and Native. All of the error terms along the Q-Q plot line for each of the ethnicity variables followed a curve with large left and right tails. Also, there were not enough responses from the Native ethnicity to construct a meaningful boxplot.  For the native Q-Q plot, there was a clear pattern of the error terms along the line implying non-normality. Therefore, based on the assessment of the boxplots, histograms, and Q-Q plots, none of the ethnicities appear normally distributed.
# Correlations


```{r DF and cor Setup} 
dat1_varOfInterest <- dat1 %>% select(colnames(full_2015_varOfInterest))

full_2015_cor <- cor(dat1_varOfInterest, use = "complete.obs")

loadPkg("corrplot")
corrplot(full_2015_cor)
```

```{r Subsetting, include=FALSE} 
Freedom_2015 <- subset(dat1_varOfInterest, select = c(IncomePerCap, OverallFreedom, EconomicFreedom, PersonalFreedom, RegulatoryPolicy, FiscalPolicy))

Work_2015 <- subset(dat1_varOfInterest, select = c(IncomePerCap, Professional, Service, Office, Construction, Production, SelfEmployed, Unemployment))

Ethnic_2015 <- subset(dat1_varOfInterest, select = c(IncomePerCap, Hispanic, White, Black, Native, Asian))
```

```{r Cor w/i Freedom}
Freedom_2015_cor <- cor(Freedom_2015, use = "complete.obs")
#corrplot(Freedom_2015_cor)
corrplot.mixed(Freedom_2015_cor, tl.pos = "lt")
```


```{r Cor w/i Work}
Work_2015_cor <- cor(Work_2015, use = "complete.obs")
#corrplot(Work_2015_cor)
corrplot.mixed(Work_2015_cor, tl.pos = "lt")
```

```{r Cor w/i Ethnic Background}
Ethnic_2015_cor <- cor(Ethnic_2015, use = "complete.obs")
#corrplot(Ethnic_2015_cor)
corrplot.mixed(Ethnic_2015_cor, tl.pos = "lt")
```

# Scatter PLots

```{r}
ggplot(dat1, aes( x= Professional, y = IncomePerCap, color = White)) + 
  geom_point(size=0.1) + geom_smooth(method='lm', formula= IncomerPerCap~Professional) +
  labs (title="Scatter Plot of IncomePerCap and Professional",
        x="Professional",
        y = "IncomePerCap")

ggplot(dat1, aes( x= Unemployment, y = IncomePerCap, color = White)) + 
  geom_point(size=0.1) + geom_smooth(method='lm', formula= IncomerPerCap~Unemployment) +
  labs (title="Scatter Plot of IncomePerCap and Unemployment",
        x="Unemployment",
        y = "IncomePerCap")


ggplot(dat1, aes( x= Service, y = IncomePerCap, color = White)) + 
  geom_point(size=0.1) + geom_smooth(method='lm', formula= IncomerPerCap~Service) +
  labs (title="Scatter Plot of IncomePerCap and Service",
        x="Service",
        y = "IncomePerCap")


ggplot(dat1, aes( x= EconomicFreedom, y = IncomePerCap, color = White)) + 
  geom_point(size=0.1) + geom_smooth(method='lm', formula= IncomerPerCap~EconomicFreedom) +
  labs (title="Scatter Plot of IncomePerCap and EconomicFreedom",
        x="EconomicFreedom",
        y = "IncomePerCap")
```


# ANOVA



```{r Majority Ethnicities Dataset} 
rowMaxs <- function(df, na.rm=TRUE) {

  if (is.matrix(df)) {df <- data.frame(df, stringsAsFactors=FALSE, drop = FALSE)}

  valid.cols <- sapply(df, function(x) { is.numeric(x) || is.logical(x) || is.character(x)})
  stopifnot(any(valid.cols))
  # or could just return NA?:
  # if (!any(valid.cols)) {return(NA)}
  if (any(!valid.cols) ) {warning('using only numeric (double or integer) or logical or character columns -- ignoring other columns ')}

  result <- do.call(pmax, c(df[ , valid.cols, drop = FALSE], na.rm=na.rm))

  result[nononmissing <- rowSums(!is.na(df[ , valid.cols, drop = FALSE]))==0] <- -Inf
  if (any(nononmissing)) {warning('where no non-missing arguments, returning -Inf')}
  return(result)

  # df = data.frame of numeric values, i.e. a list of vectors passed to pmax
  # Value returned is vector, each element is max of a row of df
}

dat1_varOfInterest1 <- dat1_varOfInterest %>% mutate(EthnicMajority = case_when(
  White >= 50 ~ "White",
  Hispanic >= 50 ~ "Hispanic",
  Black >= 50 ~ "Black",
  Native >= 50 ~ "Native",
  Asian >= 50 ~ "Asian",
  TRUE ~ "Diverse"))
EthnicityMaxes <- rowMaxs(dat1_varOfInterest1 %>%  select(White, Hispanic, Black, Native, Asian))
WorkMaxes <- rowMaxs(dat1_varOfInterest1 %>%  select(Professional:Unemployment))
dat1_varOfInterest2 <- cbind(dat1_varOfInterest1, EthnicityMaxes, WorkMaxes)

dat1_varOfInterest_withMajorityEthnicity <- dat1_varOfInterest2 %>% mutate(EthnicPlurality = case_when(
  White == EthnicityMaxes ~ "White",
  Hispanic == EthnicityMaxes ~ "Hispanic",
  Black == EthnicityMaxes ~ "Black",
  Native == EthnicityMaxes ~ "Native",
  Asian == EthnicityMaxes ~ "Asian",
  TRUE ~ "Error"),
  WorkPlurality = case_when(
  Professional == WorkMaxes ~ "Professional",
  Service == WorkMaxes ~ "Service",
  Office == WorkMaxes ~ "Office",
  Construction == WorkMaxes ~ "Construction",
  Production == WorkMaxes ~ "Production",
  SelfEmployed == WorkMaxes ~ "SelfEmployed",
  Unemployment == WorkMaxes ~ "Unemployment",
  TRUE ~ "Error"))
dat1_varOfInterest_withMajorityEthnicity$EthnicMajority <-
  as.factor(dat1_varOfInterest_withMajorityEthnicity$EthnicMajority)
dat1_varOfInterest_withMajorityEthnicity$EthnicPlurality <-
  as.factor(dat1_varOfInterest_withMajorityEthnicity$EthnicPlurality)
dat1_varOfInterest_withMajorityEthnicity$WorkPlurality <-
  as.factor(dat1_varOfInterest_withMajorityEthnicity$WorkPlurality)
```

```{r dataset} 
anova_dat <- dat1_varOfInterest_withMajorityEthnicity %>% filter(EthnicPlurality!= "Error",
                                                                 WorkPlurality!= "Error")
```

```{r Ethnic ANOVA}
Ethnic_2015_anova = aov(IncomePerCap ~ EthnicPlurality, data = anova_dat)
Ethnic_2015_anova
names(Ethnic_2015_anova)
summary(Ethnic_2015_anova)
```

```{r Ethnic Graph} 
ggplot(anova_dat, aes(x=EthnicPlurality, y=IncomePerCap)) + 
  geom_boxplot(outlier.shape=8, outlier.size=4) +
  labs(title="Income/Capita with Different Ethnic Majority", x="Ethnic Majority", y = "Income per Cap")
```

```{r Ethnic Tukey, include=FALSE}
TukeyHSD(Ethnic_2015_anova)
```

```{r Work ANOVA} 
Work_2015_anova = aov(IncomePerCap ~ WorkPlurality, data = anova_dat)
Work_2015_anova
names(Work_2015_anova)
summary(Work_2015_anova)
```

```{r Work Graph}
ggplot(anova_dat, aes(x=WorkPlurality, y=IncomePerCap)) + 
  geom_boxplot(outlier.shape=8, outlier.size=4) +
  labs(title="Income/Capita with Different Work Majority", x="Work Majority", y = "Income per Cap")
```

```{r Work Tukey, include=FALSE}
TukeyHSD(Work_2015_anova)
```

# Chi Squared Test of Independence

```{r Chi Squared Test} 
loadPkg('sjPlot')
loadPkg('visualize')

chi_table_dat <- dat1_varOfInterest_withMajorityEthnicity %>%
  select(c(WorkPlurality, EthnicPlurality)) %>% filter(WorkPlurality != "Error")

chi_table_dat %>%
  sjtab(fun = "xtab", var.labels=c("Work", "Ethnicity"),
       show.summary=T, show.exp=T, show.legend=T)
```

	A Chi Squared test of independence was used to further investigate the relationship between plurality work type and plurality ethnicity.  With a p-value below the table’s printing threshold, the test strongly indicates that there is a strong relationship between these two variables .  As noted above, there are many more tracts that are both plurality white and where the largest work type is professional than would be expected due to chance. Note that when a tract has a plurality of ethnicity X and work type Y, it doesn’t necessarily mean that the workers of X ethnicity are working in positions of type Y. For instance, consider a case where 60% of the people in a given census tract are black, and they are evenly split between service work and construction, and the remaining 40% of the population are white  and they work in professional jobs. In this imaginary and extremely segregated census tract, the plurality ethnicity is black and the plurality work type is professional, despite the fact that no black residents are working professional jobs. Thus, we must be careful not to overstate the implications for which individuals are working in which jobs.


# Regression
  Variables from the dataset were chosen to perform the regression. Using measures such as R-square, Adjusted R-Square, Complexity Parameter (CP) , Bayesian Information Criterion (BIC) and Residual Sum of Square (RSS) the best variable that could fit the model was selected. To begin with different measures v/s number of variables were plotted.
##Exhaustive Search

```{r} 
loadPkg('leaps')
#This is essentially best fit 
reg.best10 <- regsubsets(IncomePerCap~. , data = dat1_varOfInterest, nvmax = 17)  
plot(reg.best10, scale = "adjr2", main = "Adjusted R^2")
plot(reg.best10, scale = "r2", main = "R^2")
# In the "leaps" package, we can use scale=c("bic","Cp","adjr2","r2")
plot(reg.best10, scale = "bic", main = "BIC")
plot(reg.best10, scale = "Cp", main = "Cp")
reg.summary<-summary(reg.best10)
names(reg.best10)
plot(reg.summary$rsq,xlab = "# of Varibales", ylab = "Rsquare", type = "l")
plot(reg.summary$rss,xlab = "# of Varibales", ylab = "RSS", type = "l")
#Adjusted R2
which.max(reg.summary$adjr2)
plot(reg.summary$adjr2,xlab = '# of Varibales', ylab = 'Adjusted Rsq', type = "l")
points(12,reg.summary$adjr2[12],col = "red",cex = 2, pch =20)
#CP
which.min(reg.summary$cp)
plot(reg.summary$cp,xlab = '# of Varibales', ylab = 'CP', type = "l")
points(12,reg.summary$cp[12],col = "red",cex = 2, pch =20)
#BIC
which.min(reg.summary$bic)
plot(reg.summary$bic,xlab = '# of Varibales', ylab = 'BIC', type = "l")
points(10,reg.summary$bic[12],col = "red",cex = 2, pch =20)

```
  From the distribution graph, almost all the variables are included which was not helpful. If we were to select the best variables then it would be ‘WHITE’, 'Native', 'Asian','‘Professional’,'Office','Construction','Production',‘Unemployment’, 'Fiscal Policy'‘Personal Freedom’ and ‘service’ which is at highest R2 Value .68. 
Adjusted R2 - The highest adjusted R2 is obtained at .68 and this is similar to that of R2, the only difference is the variable ‘Service’ is excluded.
For BIC and CP, the lowest values are 12 and 10 and they are obtained when we include 12 and 8 variables in the regression


## Forward Selection
  
```{r}
reg.forward10 <- regsubsets(IncomePerCap~. , data = dat1_varOfInterest, nvmax = 17, nbest = 2, method = "forward")
plot(reg.forward10, scale = "adjr2", main = "Adjusted R^2")
plot(reg.forward10, scale = "bic", main = "BIC")
plot(reg.forward10, scale = "Cp", main = "Cp")
#summary(reg.forward10)
```
  R2 is not used as a criteria , which usually improves with number of variables and leads to overfitting.
For Adjusted R2,we could choose the range of  best variables from 4 to 10. 
The best variables are ‘WHITE’, ‘Professional’, ‘Unemployment’, ‘Personal Freedom’, ‘service’ which is at highest R2 Value .68. 
For BIC and CP, the lowest measure value variables are 12 and 10 respectively.

## Backward Selection

```{r}
reg.back10 <- regsubsets(IncomePerCap~. , data = dat1_varOfInterest, nvmax = 17, nbest = 2, method = "backward")
plot(reg.back10, scale = "adjr2", main = "Adjusted R^2")
plot(reg.back10, scale = "bic", main = "BIC")
plot(reg.back10, scale = "Cp", main = "Cp")
#summary(reg.back10)
```
  Now backwards (`nvmax=17` and `nbest=2`)
Best variables from the range of 9 to 11 could be choosen and they are ‘WHITE’, 'Native', 'Asian','‘Professional’,'Office','Construction', 'Production',‘Unemployment’, 'Fiscal Policy'‘Personal Freedom’ and ‘service’.For BIC and CP, the lowest measure value variables are 12 and 10 respectively.

## Sequential Replacement seqrep  
  
```{r} 
reg.seqrep <- regsubsets(IncomePerCap~. , data = dat1_varOfInterest, nvmax = 17, nbest = 2 , method = "seqrep")
plot(reg.seqrep, scale = "adjr2", main = "Adjusted R^2")
plot(reg.seqrep, scale = "bic", main = "BIC")
plot(reg.seqrep, scale = "Cp", main = "Cp")
#summary(reg.seqrep)
```
  Lastly Sequential Replacement. 
How accurate and precise are these models ? 
we don’t know yet, until we run some validation set approach or cross validations.

# Conclusion
	Overall, this analysis found that there are several ways in which our independent variables reliably predict income in communities across the United States. The Freedom variables we drew from the Cato Institute performed poorest, with a high internal correlation and little predictive power. Ethnicity and work type proportions had stronger predictive power, with the latter having the most powerful effects.  However, these variables suffer from being largely non-normal, with a rightward skew, and from having high internal correlations, both between and within the two categories. Altogether, these variables allow us to predict income per capita at the census tract level with high reliability (R-squared = .67); this is actually quite impressive given the simplicity of this data. For instance, it does not directly include any information about the age or education of the population.
	Moving forward, this analysis allows for several expansions. The first is to integrate new data, such as age and education status of census tract residents. Additionally, it may be valuable to consider each of the individual freedom measures on its own, to negate the influence of high internal correlation. Finally, it is interesting if there are differences driven by geographic density, which can be estimated with just the currently accessible data.
	
# Bibliography

Cato Institute. (2018) Freedom In the Fifty States. UpToDate. Retrieved March 23, 2020, from https://www.freedominthe50states.org/how-its-calculated

MuonNeutrino. (2015). US Census Demographic Data: Demographic and Economic Data for Tracts and Counties. UpToDate. Retrieved March 23, 2020, from https://www.kaggle.com/muonneutrino/us-census-demographic-dataD

U.S. Census Bureau (2019). "Annual Estimates of the Resident Population for the United States, Regions, States, and Puerto Rico: April 1, 2010 to July 1, 2019". 2010-2019 Population Estimates. United States Census Bureau, Population Division. December 30, 2019. Retrieved January 27, 2020.

U.S. Census Bureau (2017).   "American FactFinder - Results". U.S. Census Bureau. Retrieved 2017-12-13.

U.S. Census Bureau (2013). "2010 Census Summary File 1: GEOGRAPHIC IDENTIFIERS". American Factfinder. US Census. Retrieved 18 October 2013.
