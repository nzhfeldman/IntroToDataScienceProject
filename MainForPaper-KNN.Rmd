---
title: "Group Project"
author: "Dinesh Padmanabhan, Jake Lieberfarb,
Luis Ahumada, Noah Feldman, Becca Blacker
"
date: "3/09/2020"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(warning = F, results = T, message = T, echo = F)
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```


```{r Functions, include=FALSE} 
loadPkg = function(pkg, character.only = FALSE) { 
  if (!character.only) { pkg <- as.character(substitute(pkg)) }
  if (!require(pkg,character.only=T, quietly =T)) {  install.packages(pkg,dep=T,repos="http://cran.us.r-project.org"); if(!require(pkg,character.only=T)) stop("Package not found") } 
}
loadPkg(knitr)
```

```{r Remove Outliers, include=FALSE} 
# Fix outliers

outlierKD2 <- function(df, var, rm=FALSE) { 
    #' Original outlierKD functino by By Klodian Dhana,
    #' https://www.r-bloggers.com/identify-describe-plot-and-remove-the-outliers-from-the-dataset/
    #' Modified to have third argument for removing outliers inwtead of interactive prompt, 
    #' and after removing outlier, original df will not be changed. The function returns the new df, 
    #' which can be saved as original df name if desired.
    #' Check outliers, and option to remove them, save as a new dataframe. 
    #' @param df The dataframe.
    #' @param var The variable in the dataframe to be checked for outliers
    #' @param rm Boolean. Whether to remove outliers or not.
    #' @return The dataframe with outliers replaced by NA if rm==TRUE, or df if nothing changed
    #' @examples
    #' outlierKD2(mydf, height, FALSE)
    #' mydf = outlierKD2(mydf, height, TRUE)
    #' mydfnew = outlierKD2(mydf, height, TRUE)
    dt = df # duplicate the dataframe for potential alteration
    var_name <- eval(substitute(var),eval(dt))
    na1 <- sum(is.na(var_name))
    m1 <- mean(var_name, na.rm = T)
    #par(mfrow=c(2, 2), oma=c(0,0,3,0))
    boxplot(var_name, main="With outliers")
    hist(var_name, main="With outliers", xlab=NA, ylab=NA)
    outlier <- boxplot.stats(var_name)$out
    mo <- mean(outlier)
    var_name <- ifelse(var_name %in% outlier, NA, var_name)
    boxplot(var_name, main="Without outliers")
    hist(var_name, main="Without outliers", xlab=NA, ylab=NA)
    title("Outlier Check", outer=TRUE)
    na2 <- sum(is.na(var_name))
    cat("Outliers identified:", na2 - na1, "\n")
    cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(var_name))*100, 1), "\n")
    cat("Mean of the outliers:", round(mo, 2), "\n")
    m2 <- mean(var_name, na.rm = T)
    cat("Mean without removing outliers:", round(m1, 2), "\n")
    cat("Mean if we remove outliers:", round(m2, 2), "\n")
    
    # response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
    # if(response == "y" | response == "yes"){
    if(rm){
        dt[as.character(substitute(var))] <- invisible(var_name)
        #assign(as.character(as.list(match.call())$dt), dt, envir = .GlobalEnv)
        cat("Outliers successfully removed", "\n")
        return(invisible(dt))
    } else {
        cat("Nothing changed", "\n")
        return(invisible(df))
    }
}

loadPkg("car")
```

```{r Load 2015 data, include=FALSE}
loadPkg("plyr")
loadPkg('dplyr')
loadPkg('readxl')
loadPkg('readr')

#Read in general census data
census_2015 <- read.csv(file = 'acs2015updated.csv')


```

```{r population, include=FALSE} 
loadPkg("ggplot2")

# NA's
dat0<-census_2015[!is.na(census_2015$IncomePerCap),]
sum(is.na(dat0$IncomePerCap))

#removed outliers 
dat1 <- outlierKD2(dat0, IncomePerCap, TRUE)
#Histogram of dependent variable
hist(dat1$TotalPop, 
        col = "#a8c2fb",
        main = "population Count",
        xlab = "US Dollars")

#ggplot
qqnorm(dat1$TotalPop, main="Q-Q plot of TotalPop")
qqline(dat1$TotalPop)

#Mean after removing NA's
format(mean(dat1$TotalPop, na.rm = TRUE))

#Summary after removing NA's
summary(dat1$TotalPop)
#standard deviation
sd(dat1$TotalPop, na.rm = TRUE)

```
  A baseline analysis of population and income was conducted. The histogram for population appeared skewed to the right. The different census tracts had similar population counts with a mean of about 4000. Counties were not evenly spread out as some had a population of 1 million and others 10 million. With similar populations, census tracts were easier to investigate instead of counties. The Q-Q plot confirmed the non-normality as the values between quartiles 3 and 4 were far away from the line. 
```{r income including outliers, include=FALSE} 
#Histogram of dependent variable
hist(dat0$IncomePerCap, 
        col = "#a8c2fb",
        main = "Income per Capita",
        xlab = "US Dollars")

#ggplot
qqnorm(dat0$IncomePerCap, main="Q-Q plot of IncomePerCap")
qqline(dat0$IncomePerCap)

#Mean after removing NA's
format(mean(dat0$IncomePerCap, na.rm = TRUE))

#Summary after removing NA's
summary(dat0$IncomePerCap)
#standard deviation
sd(dat0$IncomePerCap, na.rm = TRUE)
```



## Income Histogram and QQ
```{r income no outliers} 

#Histogram of dependent variable
hist(dat1$IncomePerCap, 
        col = "#a8c2fb",
        main = "Income per Capita",
        xlab = "US Dollars")

#ggplot
qqnorm(dat1$IncomePerCap, main="Q-Q plot of IncomePerCap")
qqline(dat1$IncomePerCap)


#Total NA Before
sum(is.na(dat1$IncomePerCap))

#Remove NA
dat1 <- dat1[!is.na(dat1$IncomePerCap), ]

#Total NA After
sum(is.na(dat1$IncomePerCap))
length(dat1$IncomePerCap)


#Summary after removing NA's
summary(dat1$IncomePerCap)
#standard deviation
sd(dat1$IncomePerCap, na.rm = TRUE)


#Histogram of dependent variable
hist(dat1$IncomePerCap, 
        col = "#a8c2fb",
        main = "Income per Capita",
        xlab = "US Dollars")

#ggplot
qqnorm(dat1$IncomePerCap, main="Q-Q plot of IncomePerCap")
qqline(dat1$IncomePerCap)




```
  The raw data for income appeared very skewed to the right as well. The data appeared to follow a power-law curve as some individuals have amassed a large amount of income and these outliers can skew the data. Thus, the outliers and NA values were removed; checking again, the “cleaned data” appeared normal. The histogram appears monomodal and the error terms along the Q-Q plot did not stray away from the line.


```{r Dividing Income in 2 groups} 


loadPkg("dvmisc")



#ipc
ipc <- quant_groups(dat1$IncomePerCap, 2)
str(ipc)



#Boxplot of ipc
plot(IncomePerCap ~ ipc, data=dat1, main="IncomePerCap", col=c("#ffd18b", "#ff875f") )

#Adding new column to dat1
dat1 <- cbind(dat1, ipc)

dat1$ipc <- as.factor(dat1$ipc)


```


```{r}
#Drop columns

dat1 <- subset(dat1, select = -c(FamilyWork, SelfEmployed, SelfEmployed, PublicWork, PrivateWork, Employed, MeanCommute, WorkAtHome, OtherTransp, Walk, Transit, Carpool, Drive, ChildPoverty, Poverty, IncomePerCapErr, IncomeErr, Income, Citizen, Native, Pacific, Women, Men, County, State, INTPTLONG, INTPTLAT, Comparision, GEOID, CensusTract, IncomePerCap))
               
str(dat1)
```




```{r}


#Total NA Before
sum(is.na(dat1))

#Remove NA
dat1 <- na.omit(dat1)

#Total NA After
sum(is.na(dat1))
length(dat1)

str(dat1)

```


# KNN


**Pairs**  

```{r}
#loadPkg(psych)

#pairs.panels(dat1[,-3], 
#             method = "pearson", # correlation method
#             hist.col = "#00AFBB", # set histogram color, can use "#22AFBB", "red",
#             density = TRUE,  # show density plots
#            ellipses = TRUE # show correlation ellipses
#             )
#unloadPkg(psych)
```

## KNN  

**Train-Test split 3:1**  

```{r}
loadPkg(FNN)
set.seed(1000)
scaleddat <- as.data.frame(scale(dat1[0:11], center = TRUE, scale = TRUE))
dat_sample <- sample(2, nrow(scaleddat), replace=TRUE, prob=c(0.67, 0.33))


dat_training <- scaleddat[dat_sample==1, 0:11]
dat_test <- scaleddat[dat_sample==2, 0:11]

dat.trainLabels <- dat1[dat_sample==1, 12]
dat.testLabels <- dat1[dat_sample==2, 12]

```


**KNN results**  
```{r}
#So now we will deploy our model 
dat_pred <- knn(train = dat_training, test = dat_test, cl=dat.trainLabels, k=3)
#dat_pred
#install.packages("gmodels")
#loadPkg(gmodels)
#datPREDCross <- CrossTable(dat.testLabels, dat_pred, prop.chisq = FALSE)
#Looks like we got all but three correct, not bad

# View the output.
#str(dat_pred)
#length(dat_pred)
#table(dat_pred)
```


```{r}
# How does the kNN classification compare to the true class?
# Let's take a look at the confusion matrix by combining the 
# predictions from bank_3NN to the original data set.
loadPkg(class)


kNN_res = table(dat_pred,
                dat.testLabels)
kNN_res
sum(kNN_res)  #<- the total is all the test examples

# Select the true positives and true negatives by selecting
# only the cells where the row and column names are the same.
kNN_res[row(kNN_res) == col(kNN_res)]

# Calculate the accuracy rate by dividing the correct classifications
# by the total number of classifications.
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)
kNN_acc
```



```{r}
loadPkg(caret) 
cm = confusionMatrix(dat_pred, reference = dat.testLabels)
cm$overall
cm$byClass
```

### Selecting the correct "k"
How does "k" affect classification accuracy? Let's create a function to calculate classification accuracy based on the number of "k."
```{r}
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k) #,                #<- number of neighbors considered
                  # use.all = TRUE)       #<- control ties between class assignments
                                        #   If true, all distances equal to the kth 
                                        #   largest are included
  
  tab = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(tab[row(tab) == col(tab)]) / sum(tab)                         
  cbind(k = k, accuracy = accu)
}

# The sapply() function plugs in several values into our chooseK function.
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop.
knn_different_k = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = scaleddat[dat_sample==1, 0:11],
                                             val_set = scaleddat[dat_sample==2, 0:11],
                                             train_class = dat1[dat_sample==1, 12],
                                             val_class = dat1[dat_sample==2, 12]))


# Reformat the results to graph the results.
str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg(ggplot2)

ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)

```

```{r, include=FALSE}
# unloadPkg(???)
```



# Conclusion
	Overall, this analysis found that there are several ways in which our independent variables reliably predict income in communities across the United States. The Freedom variables we drew from the Cato Institute performed poorest, with a high internal correlation and little predictive power. Ethnicity and work type proportions had stronger predictive power, with the latter having the most powerful effects.  However, these variables suffer from being largely non-normal, with a rightward skew, and from having high internal correlations, both between and within the two categories. Altogether, these variables allow us to predict income per capita at the census tract level with high reliability (R-squared = .67); this is actually quite impressive given the simplicity of this data. For instance, it does not directly include any information about the age or education of the population.
	Moving forward, this analysis allows for several expansions. The first is to integrate new data, such as age and education status of census tract residents. Additionally, it may be valuable to consider each of the individual freedom measures on its own, to negate the influence of high internal correlation. Finally, it is interesting if there are differences driven by geographic density, which can be estimated with just the currently accessible data.
	
# Bibliography

Cato Institute. (2018) Freedom In the Fifty States. UpToDate. Retrieved March 23, 2020, from https://www.freedominthe50states.org/how-its-calculated

MuonNeutrino. (2015). US Census Demographic Data: Demographic and Economic Data for Tracts and Counties. UpToDate. Retrieved March 23, 2020, from https://www.kaggle.com/muonneutrino/us-census-demographic-dataD

U.S. Census Bureau (2019). "Annual Estimates of the Resident Population for the United States, Regions, States, and Puerto Rico: April 1, 2010 to July 1, 2019". 2010-2019 Population Estimates. United States Census Bureau, Population Division. December 30, 2019. Retrieved January 27, 2020.

U.S. Census Bureau (2017).   "American FactFinder - Results". U.S. Census Bureau. Retrieved 2017-12-13.

U.S. Census Bureau (2013). "2010 Census Summary File 1: GEOGRAPHIC IDENTIFIERS". American Factfinder. US Census. Retrieved 18 October 2013.
