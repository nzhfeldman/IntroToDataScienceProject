---
title: "K-meansJL"
author: "Jake Lieberfarb"
date: "4/3/2020"
output: html_document
---
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(warning = F, results = T, message = T, echo = F)
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```


```{r Functions, include=FALSE} 
loadPkg = function(pkg, character.only = FALSE) { 
  if (!character.only) { pkg <- as.character(substitute(pkg)) }
  if (!require(pkg,character.only=T, quietly =T)) {  install.packages(pkg,dep=T,repos="http://cran.us.r-project.org"); if(!require(pkg,character.only=T)) stop("Package not found") } 
}
loadPkg(knitr)
```

```{r Remove Outliers, include=FALSE} 
# Fix outliers

outlierKD2 <- function(df, var, rm=FALSE) { 
    #' Original outlierKD functino by By Klodian Dhana,
    #' https://www.r-bloggers.com/identify-describe-plot-and-remove-the-outliers-from-the-dataset/
    #' Modified to have third argument for removing outliers inwtead of interactive prompt, 
    #' and after removing outlier, original df will not be changed. The function returns the new df, 
    #' which can be saved as original df name if desired.
    #' Check outliers, and option to remove them, save as a new dataframe. 
    #' @param df The dataframe.
    #' @param var The variable in the dataframe to be checked for outliers
    #' @param rm Boolean. Whether to remove outliers or not.
    #' @return The dataframe with outliers replaced by NA if rm==TRUE, or df if nothing changed
    #' @examples
    #' outlierKD2(mydf, height, FALSE)
    #' mydf = outlierKD2(mydf, height, TRUE)
    #' mydfnew = outlierKD2(mydf, height, TRUE)
    dt = df # duplicate the dataframe for potential alteration
    var_name <- eval(substitute(var),eval(dt))
    na1 <- sum(is.na(var_name))
    m1 <- mean(var_name, na.rm = T)
    #par(mfrow=c(2, 2), oma=c(0,0,3,0))
    boxplot(var_name, main="With outliers")
    hist(var_name, main="With outliers", xlab=NA, ylab=NA)
    outlier <- boxplot.stats(var_name)$out
    mo <- mean(outlier)
    var_name <- ifelse(var_name %in% outlier, NA, var_name)
    boxplot(var_name, main="Without outliers")
    hist(var_name, main="Without outliers", xlab=NA, ylab=NA)
    title("Outlier Check", outer=TRUE)
    na2 <- sum(is.na(var_name))
    cat("Outliers identified:", na2 - na1, "\n")
    cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(var_name))*100, 1), "\n")
    cat("Mean of the outliers:", round(mo, 2), "\n")
    m2 <- mean(var_name, na.rm = T)
    cat("Mean without removing outliers:", round(m1, 2), "\n")
    cat("Mean if we remove outliers:", round(m2, 2), "\n")
    
    # response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
    # if(response == "y" | response == "yes"){
    if(rm){
        dt[as.character(substitute(var))] <- invisible(var_name)
        #assign(as.character(as.list(match.call())$dt), dt, envir = .GlobalEnv)
        cat("Outliers successfully removed", "\n")
        return(invisible(dt))
    } else {
        cat("Nothing changed", "\n")
        return(invisible(df))
    }
}

loadPkg("car")
```

```{r uzscale_fcn }
uzscale <- function(df, append=0, excl=NULL) { 
  #' Standardize dataframe to z scores, safe for non-numeric variables. 
  #' ELo 201904 GWU DATS
  #' @param df The dataframe.
  #' @param append T/F or 0/1. Option to append scaled columns or replace original columns in the dataframe.
  #' @param excl A list c(a,b,"d","ef") of excluded columns, either by their indexes and/or names.
  #' @return The transformed dataframe, appended or replaced with standardized scores. Non-numeric columns will not be appended, or if "replace option" is chosen, the columns will be untouched.
  #' @examples
  #' library("ISLR")
  #' tmp = uzscale( Hitters )
  #' tmp = uzscale( Hitters, 1 )
  #' tmp = uzscale( Hitters, TRUE, c(19,"NewLeague") )

  append = ifelse(append==TRUE || append=="true" || append=="True" || append=="T" || append=="t" || append==1 || append=="1", TRUE, FALSE) # standardize append 
  nmax = length(df)
  if (nmax < 1 || !is.numeric(nmax) ) { return(df) }
  df1 = df
  onames = colnames(df)  # the original column names
  cnames = onames  # the new column names, if needed start with the original ones
  znames = paste("z",cnames, sep="")     # new column names added prefix 'z'. Those are non-numeric will not be used.
  nadd = ifelse(append, nmax, 0) # add to the column index or replace the orig columns
  j=1  # counting index
  for( i in 1:nmax ) {
    if ( is.numeric(df[,i]) && !( i %in% excl || onames[i] %in% excl ) ) { 
      df1[,j+nadd] = scale(df[,i])
      cnames = c(cnames, znames[i])
      j=j+1
    } else if ( !append ) { j=j+1
    } # if append == 1 and (colunm non-numeric or excluded), do not advance j.
  }
  if (append) { colnames(df1) <- cnames }
  return(df1)
}
# sample
# loadPkg(ISLR)
# HittersClean = subset(Hitters, Salary != "NA")
# tmp = uzscale(HittersClean,0,c(2,"Salary") )
# detach("package:ISLR", unload = T)
```


```{r Load 2015 data, include=FALSE}
loadPkg('dplyr')
loadPkg('readxl')
loadPkg('readr')

#Read in general census data
census_2015 <- read.csv(file = 'acs2015updated.csv')

#Read in Specifically Education Data
edu_data_2015 <- read_csv("ACS_15_5YR_B15003_renamed.csv", skip = 1) %>% mutate(Id2 = as.numeric(Id2))

#Read in Freedom Data
freedom <- read_excel("Freedom_In_The_50_States_2018.xlsx", sheet = "Overall")

#Combine
census_edu_2015 <- left_join(census_2015, edu_data_2015, by = c("CensusTract" = "Id2"))
full_2015 <- full_join(census_edu_2015, freedom %>% filter(Year == 2015), by = "State") 

#For the education data, we will need to divide the count by the total
full_2015_eduProp <- full_2015 %>% mutate(PropHighSchool = EstHighSchool/EstTotal,
                                          PropBachelors = EstBachelors/EstTotal,
                                          PropDoctorate = EstDoctorate/EstTotal)
full_2015_varOfInterest <- full_2015 %>%  select(IncomePerCap,Hispanic:Asian,Professional:Production, SelfEmployed, Unemployment, FiscalPolicy,EconomicFreedom, RegulatoryPolicy, PersonalFreedom, OverallFreedom)

```


```{r population, include=FALSE} 
loadPkg("ggplot2")

# NA's
dat0<-full_2015[!is.na(full_2015$IncomePerCap),]
sum(is.na(dat0$IncomePerCap))

#removed outliers 
dat1 <- outlierKD2(dat0, IncomePerCap, TRUE)
#Histogram of dependent variable
hist(dat1$TotalPop, 
        col = "#a8c2fb",
        main = "population Count",
        xlab = "US Dollars")

#ggplot
qqnorm(dat1$TotalPop, main="Q-Q plot of TotalPop")
qqline(dat1$TotalPop)

#Mean after removing NA's
format(mean(dat1$TotalPop, na.rm = TRUE))

#Summary after removing NA's
summary(dat1$TotalPop)
#standard deviation
sd(dat1$TotalPop, na.rm = TRUE)

```


```{r income including outliers, include=FALSE} 
#Histogram of dependent variable
hist(dat0$IncomePerCap, 
        col = "#a8c2fb",
        main = "Income per Capita",
        xlab = "US Dollars")

#ggplot
qqnorm(dat0$IncomePerCap, main="Q-Q plot of IncomePerCap")
qqline(dat0$IncomePerCap)

#Mean after removing NA's
format(mean(dat0$IncomePerCap, na.rm = TRUE))

#Summary after removing NA's
summary(dat0$IncomePerCap)
#standard deviation
sd(dat0$IncomePerCap, na.rm = TRUE)
```



## Income Histogram and QQ
```{r income no outliers} 

#Histogram of dependent variable
hist(dat1$IncomePerCap, 
        col = "#a8c2fb",
        main = "Income per Capita",
        xlab = "US Dollars")

#ggplot
qqnorm(dat1$IncomePerCap, main="Q-Q plot of IncomePerCap")
qqline(dat1$IncomePerCap)


#Total NA Before
sum(is.na(dat1$IncomePerCap))

#Remove NA
dat1 <- dat1[!is.na(dat1$IncomePerCap), ]

#Total NA After
sum(is.na(dat1$IncomePerCap))
length(dat1$IncomePerCap)


#Summary after removing NA's
summary(dat1$IncomePerCap)
#standard deviation
sd(dat1$IncomePerCap, na.rm = TRUE)


#Histogram of dependent variable
hist(dat1$IncomePerCap, 
        col = "#a8c2fb",
        main = "Income per Capita",
        xlab = "US Dollars")

#ggplot
qqnorm(dat1$IncomePerCap, main="Q-Q plot of IncomePerCap")
qqline(dat1$IncomePerCap)




```


```{r}
#Subsetting
datJL <- subset(dat1, select = c(Hispanic, White, Black, Asian, Professional, Service, Office, Construction, Production, Unemployment, IncomePerCap))
               
str(datJL)
```

### Lasso and Ridge 

```{r}
loadPkg(ISLR)


#Total NA Before
sum(is.na(datJL))

#Remove NA
dat1 <- na.omit(datJL)

#Total NA After
sum(is.na(datJL))
length(datJL)

str(datJL)

datJLClean = uzscale(dat1, 0, "IncomePerCap")
```


```{r}
x=model.matrix(IncomePerCap~.,datJLClean)[,-1]
y=datJLClean$IncomePerCap

loadPkg(glmnet)
grid=10^seq(10,-2,length=100) # prepare log scale grid for λ values, from 10^10 to 10^-2, in 100 segments
ridge.mod=glmnet(x,y,alpha=0,lambda=grid) # build the ridge model. (alpha is elastic-net mixing parameter, between 0 and 1. Ridge is 0, Lasso is 1)
dim(coef(ridge.mod))  # same as dim(coefficients(ridge.mod)), is the dimensions of all the models (100 of them), and the dimension is here: 20x100
plot(ridge.mod)


#legend("topleft",x, y, legend = c("Hispanic", "White", "Black", "Asian", "Professional", "Service", "Office", "Construction", "Production", "Unemployment", "IncomePerCap"), col = c("orange", "blue", "green", "purple", "red", "violet", "aqua", "indigo", "brown", "peach", "black"))


```


```{r ridge predictions}
ridge.mod$lambda[50] # 11498
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2)) #3616
ridge.mod$lambda[60] # 705
coef(ridge.mod)[,60] 
sqrt(sum(coef(ridge.mod)[-1,60]^2)) #5092

predict(ridge.mod,s=50,type="coefficients")[1:11,]
```
## Train and Test sets

```{r, warning=F}
loadPkg(dplyr)
set.seed(1)
train = datJLClean %>% sample_frac(0.5)
test = datJLClean %>% setdiff(train)

x_train = model.matrix(IncomePerCap~., train)[,-1]
x_test = model.matrix(IncomePerCap~., test)[,-1]

y_train = train %>% select(IncomePerCap) %>% unlist() # %>% as.numeric()
y_test = test %>% select(IncomePerCap) %>% unlist() # %>% as.numeric()



```

```{r cross validation}
set.seed(1)
cv.out.ridge=cv.glmnet(x_train,y_train,alpha=0)  # Fit ridge regression model on training data
plot(cv.out.ridge)
bestlam.ridge = cv.out.ridge$lambda.min  # Select lamda that minimizes training MSE
bestlam.ridge
cat("lowest lamda from CV: ", bestlam.ridge, "\n\n")
ridge.pred=predict(ridge.mod,s=bestlam.ridge,newx=x_test)
ridgeMeanMse = mean((ridge.pred-y_test)^2)
cat("Mean MSE for best Ridge lamda: ", ridgeMeanMse, "\n\n")
#
out.ridge=glmnet(x,y,alpha=0)
ridge_coef = predict(out.ridge,type="coefficients",s=bestlam.ridge)[1:11,]
cat("\nAll the coefficients : \n")
ridge_coef
RidgeR2 <- max(1 - cv.out.ridge$cvm/var(y))
RidgeR2

rss2 <- sum((ridge.pred-y_test) ^ 2)
tss2 <- sum((y_test - mean(y_test)) ^ 2)
rsq2 <- 1 - rss2/tss2
rsq2
```
##Lasso 
```{r lasso}
lasso.mod=glmnet(x_train,y_train,alpha=1,lambda=grid)
plot(lasso.mod)
#legend("topleft",x, y, legend = c("Hispanic", "White", "Black", "Asian", "Professional", "Service", "Office", "Construction", "Production", "Unemployment", "IncomePerCap"), col = c("orange", "blue", "green", "purple", "red", "violet", "aqua", "indigo", "brown", "peach", "black"))
set.seed(1)
cv.out.lasso=cv.glmnet(x_train,y_train,alpha=1)
plot(cv.out.lasso)
bestlam.lasso=cv.out.lasso$lambda.min
cat("lowest lamda from CV: ", bestlam.lasso, "\n\n")
lasso.pred=predict(lasso.mod,s=bestlam.lasso,newx=x_test)
#
out.lasso = glmnet(x, y, alpha = 1, lambda = grid) # Fit lasso model on full dataset
lassoMeanMse = mean((lasso.pred-y_test)^2)
cat("Mean MSE for best Lasso lamda: ", lassoMeanMse, "\n\n")
#
lasso_coef = predict(out.lasso, type = "coefficients", s = bestlam.lasso)[1:11,] # Display coefficients using λ chosen by CV
cat("\nAll the coefficients : \n")
lasso_coef
cat("\nThe non-zero coefficients : \n")
lasso_coef[lasso_coef!=0]
lassoR2 <- max(1 - cv.out.lasso$cvm/var(y))
lassoR2
lo
actual <- x_test$actual
preds <- x_test$predicted
rss <- sum((lasso.pred-y_test) ^ 2)
tss <- sum((y_test - mean(y_test)) ^ 2)
rsq <- 1 - rss/tss
rsq
```
says 8 but has 9 most likely bc Hispanic has low coefficient. The effect of white and professional is much stronger than the other coefficients.
e^5.5 =
```{r lasso vs. regression}
####I want to test to see which one is better 
OLS <- lm(IncomePerCap~.,data = datJLClean)
summary(OLS)
#https://h1ros.github.io/posts/lasso-regression/
library(tidyverse)
library(caret)
library(glmnet)
models <- list(ridge = ridge.mod, lasso = lasso.mod, OLS = OLS)
resamples(models) %>% summary( metric = "RMSE")
```




##K- Means
```{r analysis for ethnicities }
loadPkg(tidyverse)  # data manipulation
loadPkg(cluster)    # clustering algorithms
loadPkg(factoextra)
loadPkg(fpc)
# include work variables {look at luis'} 
#ethnicities clustering 
#White

#down fpc https://cran.r-project.org/web/packages/fpc/index.html
#need help downloading 
# cool website https://stats.stackexchange.com/questions/31083/how-to-produce-a-pretty-plot-of-the-results-of-k-means-cluster-analysis
#k-2
set.seed(1000)
k2 <- kmeans(datJLClean, centers = 2, nstart = 25)
str(k2)
k2
clusplot(datJLClean, k2$cluster, color=TRUE, shade=TRUE,
   labels=0, lines=0)
#k-3
set.seed(1000)
k3 <- kmeans(datJLClean, centers = 3, nstart = 25)
str(k3)
k3
clusplot(datJLClean, k3$cluster, color=TRUE, shade=TRUE,
   labels=0, lines=0)
#k-4
set.seed(1000)
k4 <- kmeans(datJLClean, centers = 4, nstart = 25)
str(k4)
k4
clusplot(datJLClean, k4$cluster, color=TRUE, shade=TRUE,
   labels=0, lines=0)
#k-5
set.seed(1000)
k5 <- kmeans(datJLClean, centers = 5, nstart = 25)
str(k5)
k5
clusplot(datJLClean, k5$cluster, color=TRUE, shade=TRUE,
   labels=0, lines=0)
#k-6
set.seed(1000)
k5 <- kmeans(datJLClean, centers = 5, nstart = 25)
str(k5)
k5
clusplot(datJLClean, k5$cluster, color=TRUE, shade=TRUE,
   labels=0, lines=0)
#k-6
k6 <- kmeans(datJLClean, centers = 6, nstart = 25)
str(k6)
k6
clusplot(datJLClean, k6$cluster, color=TRUE, shade=TRUE,
   labels=0, lines=0)
#-7
k7 <- kmeans(datJLClean, centers = 7, nstart = 25)
str(k7)
k7
clusplot(datJLClean, k7$cluster, color=TRUE, shade=TRUE,
   labels=0, lines=0)
#k-8
k8 <- kmeans(datJLClean, centers = 8, nstart = 25)
str(k8)
k8
clusplot(datJLClean, k8$cluster, color=TRUE, shade=TRUE,
   labels=0, lines=0)
#k-9
k9 <- kmeans(datJLClean, centers = 9, nstart = 25)
str(k9)
k9
clusplot(datJLClean, k9$cluster, color=TRUE, shade=TRUE,
   labels=0, lines=0)
#k-10
k10 <- kmeans(datJLClean, centers =10, nstart = 25)
str(k10)
k10
clusplot(datJLClean, k10$cluster, color=TRUE, shade=TRUE,
   labels=0, lines=0)
kplot <-data.frame("K-clusters"= c(1,2,3,4,5,6,7,8,9,10), "Within cluster sum of squares by cluster:" = c(65.8,83.3,90.2,93.5,93.5,95.4,96.6,97.3,99.7,98.2))
plot(kplot,"K-clusters","Within cluster sum of squares by cluster", type ="b")


```


